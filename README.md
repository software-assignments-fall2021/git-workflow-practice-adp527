#GPT-3 and the Turing Test: How close are language models to human?
This is an article I read recently about givng GPT-3 a turing test: https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html  
GPT-3 is the latest natural language processing model from OpenAI, and even a year after its release, it remains one of the most capable language-building language models. There are several ethical concerns about this technology that aren't the focus of this article, such as inheriting biases from its training data, or being used to replace journalists or software engineers. But one question is how good is GPT-3 at pretending to be a human? This article exposes several fascinating patterns behind the logic of GPT-3's "thought process". For example, it struggles to recognize an illogical question, to remember several details or to process basic mathematic/logical questions.